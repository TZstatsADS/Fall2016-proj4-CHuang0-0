---
title: "Project 4 Description"
author: "Chenxi Huang (ch3129)"
date: "November 20, 2016"
output: word_document
---

```{r setup, include=FALSE}
This Document is use to introduce my ideas, thoughts and results of my project 4.
```

```{r}
Project: Words 4 Music
Term: Fall 2016

Contributor name: Chenxi Huang
Contributor UNI: ch3129
```

```{r}
[Project Description]

![Caption for the picture.](/path/to/just like fire lyric.png)

Projec title: Million Song
Project Goals: 

In this project we will explore the association between music features and lyrics words,
from a subset of songs in the million song data. 
Association rule mining has a wide range of applications that include 
marketing research (on co-purchasing), natural language processing, finance, public health and etc. 
Here the word "rules" is really as general as any interesting and meaningful patterns. 
Based on the association patterns identified, 
we will create lyric words recommender algorithms for a piece of music (using its music features).

```

```{r}
Project summary: 
  
After intensive exploring, I tried out three methods. 
1. Baseline, 
2. Clustering,
3. Topic Modeling. 
Suprisingly, I conclude that the Baseline Model gives me a better results using Cross Validation. 
Please see the below for more details.
```


```{r}
#################################### **My Thoughts** ####################################
1. The Data
The million song data is an array of matrices storing three groups of data: 
  Analysis, Metadata and Musicbrainz.
For the purpose of the evaluation, 
"- /metadata, -/musicbrainz, -/analysis/songs" will not be provided in the test data. 


2. The Goal
Our goal for this project is to make recommendations based on the features provided for each song, 
to determine which words are more likely to occur in this song 
and finally give ranks to all words in the Dictionary given. 


3. What Problem?
After observing the data, we can divided it into two parts: 
  1) the first part where we find relations among features; 
  2) the second part where we connect those features with distributions of words in lyr.data.


4. Features
(1) For each song, within each group, i.e. analysis, metadata and musicbrainz, 
    we calculate the 13 statistics of each covariate, using the describe() in {Psych} package, 
    which is more detailed than the summary() statistics. 
(2) For example, for the bar_confidence in the "Analysis" group, 
    we can generate compherehensive stats like
    "vars, n, mean, sd, median, trimmed, mad, min, max, range, skew, kurtosis, se".
(3) Since the "- /metadata, -/musicbrainz, -/analysis/songs" will not be provided in the test data, 
    I can hard see strong evidence suggesting that I should extract more than 13 subfeatures for each feature. 
    Overall, for the Analysis group, deducting the "songs" part, I have generated 13 * (16-1) = 195 features.
(4) Feature Selection: will be covered later. See *Details and Justifications*
  
  
5. Reason & Procedures
(1). Baseline Model. 
      Baseline model is the simplest model to be compared with other more complex models. 
      It is just determined by the frequencies of words in the lyr.Rdata file.

(2). Clustering.
     Find clusters of features, determine to which cluster each test data belongs, 
     and assign the frequencies of words in that cluster to that test data. 
     Here, I tried K Means clustering. 

(3). Topic Modeling. 
    Use Multinomial to see which topics the test data can be allocated to and their weights. 
    Use the word distributions of the topics to determine which words are more prone to occur in the test set. 


6. Details and Justifications
(1). Dimension Reduction: PCA (considered but abandoned)
     Since we have 2350 songs in the training set and 195 features, 
     reducing the dimensionality of features is important in terms of shying from overfitting. 
     Considering we are essentially doing unsupervised learning (labels of songs are noncomparable), 
     so PCA seems like a simple and good way to go. 
     
     However, PCA is reducing dimensionality but not feature selections. 
     It provided PCA components but not a subset of variables that we would like to have. 
     I abandoned this after I searched for and reading related documents for 2+ hours.

(2) Feature Selection: Random Forest(considered but abandoned)
     Random Forest, along with other classification methods, is also one of my top choices to go. 
     It selects features by their importance. 
     
     However, it contains a lot of problems, such as the lack of labels and the different scales of my features. 
     I also dropped this after more than hours of exploring. 


(3). Cross-Validation (define error = mean(predicted ranks) - mean(actual ranks in the test data))
     Cross-Validation here is used to avoid overfitting, 
     as well as an indirect criteria to determine which model is better.
     
     So far cross-validation has helped me to identify some good methods. 
     But it is also limited by its time-consuming nature. 
     Admittedly, running K=1 or 3 could lead to a totally different results as K=5.

```


```{r}
#################################### **My Findings** ####################################

1.Baseline Rankings
  The baseline is determined just by the **frequencies** of words in the *lyr.Rdata*. 
  The meaning of it is to:
    1) set up a lower bound by assuming all songs have the same word distributions, regardless of the music features;
    2) for the benefits of more complicated models, we could compare them with the baseline for future improvement.

    So the Top 20 Words annd their Frequencies in the Lyr.Rdata is:
      ![](https://github.com/TZstatsADS/Fall2016-proj4-CHuang0-0/blob/master/figs/fig1.png)

```

