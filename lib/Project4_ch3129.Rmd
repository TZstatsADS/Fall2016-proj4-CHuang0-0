---
title: "Project 4: Word 4 Music"
author: "Chenxi Huang(ch3129)"
date: "Tuesday, November 15, 2016"
output: html_document
---
```{r}
# set the workng directory 
setwd("C:/Users/celia/Desktop/Project 4")
```

```{r}
# Intro To Dataset
#Common_id.txt: ids for the songs that have both lyrics and sound analysis information. 2350 in total;
#lyr.Rdata: dim: 2350*5001. b-o-w for 2350 songs stored in a dataframe;
#data.zip: .h5 files for the 2350 songs;
#msm_dataset_train.txt original format of the lyrics data
```

```{r}
# Download Packages
#Reading hdf5 file
#Download rhdf5 library
#source("http://bioconductor.org/biocLite.R")
#biocLite("rhdf5")
# install missing packages
list.of.packages <- c("NLP", "tm","lda","LDAvis","slam","rhdf5","servr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```

```{r}
# Load Packages
library(rhdf5)

# Topic Modeling
library(NLP)
library(tm);library(slam)# slam not available for R version 3.2.3
library(lda)
library(LDAvis)
library(servr)
```

```{r}
# The structure of a song
#h5ls("/Users/Bianbian/Documents/Courses/2016Spring/4249_Applied Data Science/Proj5/MillionSongSubset/data/A/A/A/TRAAAAW128F429D538.h5")
read1=h5ls('C:/Users/celia/Desktop/Project 4/Project4_data/data/A/A/A/TRAAABD128F429CF47.h5') #try out an example
dim(read1);summary(read1);read1 #dim=27 by 5
# the data are stored in three groups, analysis, metadata and musicbrainz
# Get all data using h5read
sound=h5read("C:/Users/celia/Desktop/Project 4/Project4_data/data/A/A/A/TRAAABD128F429CF47.h5", "/analysis")
meta=h5read("C:/Users/celia/Desktop/Project 4/Project4_data/data/A/A/A/TRAAABD128F429CF47.h5", "/metadata")
musicbrainz=h5read("C:/Users/celia/Desktop/Project 4/Project4_data/data/A/A/A/TRAAABD128F429CF47.h5", "/musicbrainz")
sound
summary(sound)
meta
summary(meta)
musicbrainz
summary(musicbrainz)
# for example
sound$segments_confidence;dim(sound$segments_confidence);class(sound$segments_confidence)  
```

```{r}
######################## Topic Modeling ###########################
#Pre-processing
delcol=c(2,3,6:30)
delcol2=c(1,2,3,6:30)
stop_words <- stopwords("SMART")
stop_words;length(stop_words) #571 unuseful words

# load the lyrics
load('C:/Users/celia/Desktop/Project 4/Project4_data/lyr.Rdata')
ls() #which objects/data are available
lyr
dim(lyr) # dim=2350 * 5001
lyr[1,] # lyr[,1]=ID
lyr[1,1]
lyr.docnam=lyr[,1];lyr.docnam
lyr.dt= lyr[,-delcol2];dim(lyr.dt) #  2350 by 4973
srt1=sort(lyr.dt[1,],decreasing=T)
srt1[1:20]
names(lyr.dt) #get the word dictionary
Dic=as.vector(names(lyr.dt))
Dic;length(Dic) #4973

#check original text
# OriLyr=read.table(file.choose(),header=T)
# OriLyr
```

```{r}
# prepare documents 

term.frequency = colSums(lyr.dt)
term.frequency
length(term.frequency) #5000
# sort the frequency
term.frequency.sort=sort(term.frequency, decreasing=T)
term.frequency.sort
head(term.frequency.sort,n=20) # top 20 words
term.table =term.frequency.sort

# remove terms that are stop words or occur fewer than 5 times:
del=names(term.table) %in% stop_words | term.table < 5
del
term.table=term.table[!del]
vocab=names(term.table)           # Vocab: a vector of all words
vocab
length(vocab) # length= 3888
length(term.table) #dim= 3888
```

```{r}
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index-1), as.integer(rep(1, length(index))))
}

lyr.dt[1,];dim(lyr.dt) #dim=2350 by 4973
index_1=which(lyr.dt[1,]!=0)
name1=Dic[index_1]
name1
m1=match(name1,vocab)
m1
# test whether true
vocab[m1]
index_2=m1[!is.na(m1)]
index_2
vocab[index_2] #reduced words
lyr.dt[1,][vocab[index_2]] # number of times of each word in the document 
mat1=matrix(rbind(index_2, rep(1,length(index_2))))
class(mat1)

# write loops to generate the format for documents
dim(lyr.dt) # 2350 by 4973
lyr.dt.nrow=nrow(lyr.dt);lyr.dt.nrow #2350
vocab;length(vocab) # 3888

#empty a list to store values 
list1=list();list1
for(i in 1:lyr.dt.nrow){
  lyr1=lyr.dt[i,]
  ind1=which(lyr1!= 0)  # get all the words from the lyr data that exists in the original text
  nam1=Dic[ind1]
  match_1=match(nam1,vocab)
  ind2=match_1[!is.na(match_1)]
  word_times1=lyr1[vocab[ind2]]
  doc1=rbind(as.integer(ind2-1), as.integer(word_times1))
  list1[[i]]=doc1
 }
list1
length(list1)
list1[1]

documents=list1

```

```{r}
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents 
D #2350
W <- length(vocab)  # number of terms in the vocab 
W #3914
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
doc.length
N <- sum(doc.length)  # total number of tokens in the data (546,827)
N #545313
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, 
term.frequency

```

```{r}
# MCMC and model tuning parameters:
K <- 15
G <- 5000
alpha <- 0.1
eta <- 0.1

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop

```

